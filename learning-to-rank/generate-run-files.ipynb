{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client\n",
    "from feature_extraction import get_all_features\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lmart.pkl', 'rb') as f:\n",
    "    lmart = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = 'ows_ltr'\n",
    "RUN_DIR = Path('../runs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_run(dataset_name: str, run_suffix: str):\n",
    "    dataset = pt.get_dataset(f'irds:ir-benchmarks/{dataset_name}')\n",
    "\n",
    "    extract_features = get_all_features(tira, dataset)\n",
    "\n",
    "    bm25 = tira.pt.from_submission('ir-benchmarks/tira-ir-starter/BM25 Re-Rank (tira-ir-starter-pyterrier)', dataset)\n",
    "\n",
    "    pipeline = (bm25 % 1000) >> extract_features >> pt.ltr.apply_learned_model(lmart, form=\"ltr\")\n",
    "\n",
    "    run = pipeline(dataset.get_topics(variant='query'))\n",
    "\n",
    "    run_file = RUN_DIR / RUN_NAME / f'{RUN_NAME}.{run_suffix}'\n",
    "    run_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    pt.io.write_results(run, run_file, format='trec', run_name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    'longeval-train-20230513-training': 'train_2023', \n",
    "    'longeval-heldout-20230513-training': 'WT',\n",
    "    'longeval-short-july-20230513-training': 'ST',\n",
    "    'longeval-long-september-20230513-training': 'LT',\n",
    "    'longeval-2023-01-20240423-training': 'train_2024',\n",
    "    'longeval-2023-06-20240418-training': 'lag6',\n",
    "    'longeval-2023-08-20240418-training': 'lag8',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running longeval-train-20230513-training...\n",
      "Running longeval-heldout-20230513-training...\n",
      "Running longeval-short-july-20230513-training...\n",
      "Running longeval-long-september-20230513-training...\n",
      "Running longeval-2023-01-20240423-training...\n",
      "Running longeval-2023-06-20240418-training...\n",
      "Running longeval-2023-08-20240418-training...\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, suffix in DATASETS.items():\n",
    "    print(f'Running {dataset_name}...')\n",
    "    create_run(dataset_name, suffix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
